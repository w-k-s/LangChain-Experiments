{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b35517-907c-483a-9902-b7dc781918d1",
   "metadata": {},
   "source": [
    "# 4. Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca392679-037d-4416-a973-a99e6e33b559",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca0146d-06ea-442a-92cb-5d469339a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "assert os.environ[\"LANGSMITH_TRACING\"] is not None\n",
    "assert os.environ[\"LANGSMITH_API_KEY\"] is not None\n",
    "assert os.environ[\"LANGSMITH_PROJECT\"] is not None\n",
    "assert os.environ[\"OPENAI_API_KEY\"] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13b139db-ee39-4ba8-8136-ba2a7f440deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a23cf2-b255-4267-a180-cc4159909133",
   "metadata": {},
   "source": [
    "## 3.1 Introduction to LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c0bf5-2464-4702-b4f1-83d24e325db4",
   "metadata": {},
   "source": [
    "### 3.1.1 LLMS are Stateless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1547e-2eb2-4b37-8a48-9f16e7a6eeb8",
   "metadata": {},
   "source": [
    "- **By default, an LLM does not retain the context from previous invocations**. For example, if you tell an LLM your name in one invocation, it will not \"remember\" your name in the subsequent invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd20349d-3d8e-4302-b706-adcca18240a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "intro_response = model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n",
    "question_response = model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4a56bdb-6095-47ca-849c-f69c0dfcfad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "intro_response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c3e46f-e272-4120-a63e-8834ab07de63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name unless you tell me. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "question_response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34161437-1137-40ba-af33-039217ef501e",
   "metadata": {},
   "source": [
    "- For the LLM to remember the name, the chat history has to be sent with each invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eedf781c-9bc4-4f93-9417-cfa4b4d59f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! How can I help you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "question_response_with_history = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_response_with_history.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107160fc-95a4-4727-b7a2-84c2bc77232c",
   "metadata": {},
   "source": [
    "### 3.1.2 LangGraph Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249c25d-4a3f-4421-aef9-e21a1a1ada3a",
   "metadata": {},
   "source": [
    "- **LangGraph** is an open-source library from the creators of LangChain that is used to build A stateful, multi-step agent/workflow applications with reliable execution and persistence.\n",
    "- Importantly, Human/Agent interactions are modeled as nodes in a graph which makes the orchestration between humans and agents visible and easy to debug.\n",
    "\n",
    "#### Nodes\n",
    "- A node is a single action: e.g., call an LLM, run a tool, query a database, or invoke custom code.\n",
    "- Nodes can represent different actors (LLMs, tools, humans).\n",
    "- In case of a human, the graph encodes when to pause for human input, how to resume, and how actors exchange state.\n",
    "\n",
    "#### Edges\n",
    "- Edges connect nodes and decide what runs next.\n",
    "- They can be unconditional, conditional (branching), looping, or fan-out/fan-in (parallel paths and joins).\n",
    "- Graph branches can run in parallel\n",
    "\n",
    "#### Graph\n",
    "- A workflow is the directed graph of nodes and edges.\n",
    "- This makes the system explicit and debuggable: you can see the exact path the execution took.\n",
    "\n",
    "#### State\n",
    "- The workflow carries a state object (typically a structured dict/schema) that persists across nodes.\n",
    "- Nodes read from state and propose updates to state rather than mutating it in place\n",
    "- After each node runs, LangGraph materialises a new state version.\n",
    "- This **immutability per step** yields reproducibility, diff-ability, and clear audit trails.\n",
    "\n",
    "#### Checkpoints\n",
    "- Each step can be checkpointed (state snapshot + metadata).\n",
    "- Checkpoints and state can be stored in memory or external stores (DBs, object storage).\n",
    "\n",
    "- With checkpoints, you can retry or reroute without losing prior work.\n",
    "- This enables resume after failure/timeouts, deterministic replay, and precise debugging from any point.\n",
    "\n",
    "#### Thread\n",
    "- A thread is one concrete run of the graph (e.g., a single user session).\n",
    "- Each thread has its own state history and checkpoints, isolating concurrent users/sessions cleanly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d259c6-eceb-4638-a97f-d6d010876bb6",
   "metadata": {},
   "source": [
    "When invoking the LLM, these training examples are placed before the actual question. The model uses them as guidance and then produces its final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3cc1b-5501-49e9-9000-b085980c8b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
