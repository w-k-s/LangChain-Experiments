{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b35517-907c-483a-9902-b7dc781918d1",
   "metadata": {},
   "source": [
    "# 4. Building a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02538fce-0bdc-400e-91a4-32803f8c2664",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24495848-ccf1-456c-836f-bc80ffcd5d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca0146d-06ea-442a-92cb-5d469339a7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in ./.venv/lib/python3.13/site-packages (0.6.7)\n",
      "Requirement already satisfied: langchain-core>=0.1 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.3.69)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in ./.venv/lib/python3.13/site-packages (from langgraph) (0.2.6)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langgraph) (2.11.7)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in ./.venv/lib/python3.13/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in ./.venv/lib/python3.13/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in ./.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.venv/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (0.4.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "assert os.environ[\"LANGSMITH_TRACING\"] is not None\n",
    "assert os.environ[\"LANGSMITH_API_KEY\"] is not None\n",
    "assert os.environ[\"LANGSMITH_PROJECT\"] is not None\n",
    "assert os.environ[\"OPENAI_API_KEY\"] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b139db-ee39-4ba8-8136-ba2a7f440deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a23cf2-b255-4267-a180-cc4159909133",
   "metadata": {},
   "source": [
    "## 4.1 Introduction to LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c0bf5-2464-4702-b4f1-83d24e325db4",
   "metadata": {},
   "source": [
    "### 4.1.1 LLMS are Stateless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1547e-2eb2-4b37-8a48-9f16e7a6eeb8",
   "metadata": {},
   "source": [
    "- **By default, an LLM does not retain the context from previous invocations**. For example, if you tell an LLM your name in one invocation, it will not \"remember\" your name in the subsequent invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd20349d-3d8e-4302-b706-adcca18240a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "intro_response = model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n",
    "question_response = model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4a56bdb-6095-47ca-849c-f69c0dfcfad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "intro_response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42c3e46f-e272-4120-a63e-8834ab07de63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't know your name. If you tell me, I can use it while we chat!\n"
     ]
    }
   ],
   "source": [
    "question_response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34161437-1137-40ba-af33-039217ef501e",
   "metadata": {},
   "source": [
    "- For the LLM to remember the name, the chat history has to be sent with each invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eedf781c-9bc4-4f93-9417-cfa4b4d59f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! How can I help you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "question_response_with_history = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_response_with_history.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107160fc-95a4-4727-b7a2-84c2bc77232c",
   "metadata": {},
   "source": [
    "### 4.1.2 LangGraph Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249c25d-4a3f-4421-aef9-e21a1a1ada3a",
   "metadata": {},
   "source": [
    "- **LangGraph** is an open-source library from the creators of LangChain that is used to build A stateful, multi-step agent/workflow applications with reliable execution and persistence.\n",
    "- Importantly, Human/Agent interactions are modeled as nodes in a graph which makes the orchestration between humans and agents visible and easy to debug.\n",
    "\n",
    "#### Nodes\n",
    "- A node is a single action: e.g., call an LLM, run a tool, query a database, or invoke custom code.\n",
    "- Nodes can represent different actors (LLMs, tools, humans).\n",
    "- In case of a human, the graph encodes when to pause for human input, how to resume, and how actors exchange state.\n",
    "\n",
    "#### Edges\n",
    "- Edges connect nodes and decide what runs next.\n",
    "- They can be unconditional, conditional (branching), looping, or fan-out/fan-in (parallel paths and joins).\n",
    "- Graph branches can run in parallel\n",
    "\n",
    "#### Graph\n",
    "- A workflow is the directed graph of nodes and edges.\n",
    "- This makes the system explicit and debuggable: you can see the exact path the execution took.\n",
    "\n",
    "#### State\n",
    "- The workflow carries a state object (typically a structured dict/schema) that persists across nodes.\n",
    "- Nodes read from state and propose updates to state rather than mutating it in place\n",
    "- After each node runs, LangGraph materialises a new state version.\n",
    "- This **immutability per step** yields reproducibility, diff-ability, and clear audit trails.\n",
    "\n",
    "#### Checkpoints\n",
    "- Each step can be checkpointed (state snapshot + metadata).\n",
    "- Checkpoints and state can be stored in memory or external stores (DBs, object storage).\n",
    "\n",
    "- With checkpoints, you can retry or reroute without losing prior work.\n",
    "- This enables resume after failure/timeouts, deterministic replay, and precise debugging from any point.\n",
    "\n",
    "#### Thread\n",
    "- A thread is one concrete run of the graph (e.g., a single user session).\n",
    "- Each thread has its own state history and checkpoints, isolating concurrent users/sessions cleanly.\n",
    "\n",
    "![](./docs/langgraph-checkpoints.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb09016-e0a5-4575-895d-8a760510fe87",
   "metadata": {},
   "source": [
    "## 4.2 Building a Chatbot\n",
    "\n",
    "We'll build a simple graph. The graph will consist of:\n",
    "- A single **node** that calls an LLM\n",
    "- A **state** that is updated every time a node is called.\n",
    "- A **checkpointer** that saves the state in-memory every time the state is updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec879e7-b32f-43d9-bc99-87793f1a9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# \"model\" is an end-key. If we wand to append a node to an edge, we call add_node passing the end-key of the edge we're appending to.\n",
    "workflow.add_edge(START, \"model\")\n",
    "# Here, we want to append the call_model node to the START edge. So we pass the end-key  of the START node\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5b0c5-4b8e-46d7-9589-72572f433795",
   "metadata": {},
   "source": [
    "We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be646b0c-9678-4c98-9038-dd13f0ee14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc7789-9ed4-42af-b520-25acadcd53da",
   "metadata": {},
   "source": [
    "This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01f7ee82-a5cc-4d8e-a555-465ef733fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi again, Bob! What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd5c0478-37ce-411a-ac86-7e48bb5aacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. What’s on your mind?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc4325-0b2a-4bb5-860c-712b64ac2c44",
   "metadata": {},
   "source": [
    "If we change the thread_id, then the chatbot will no longer remember who the human is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "869cf1a1-484a-429f-8e7c-73a432abd41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal information about you unless you've shared it with me in this conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c8c17-3f2e-4c23-8d37-7269d9bf0547",
   "metadata": {},
   "source": [
    "**Every time we call the model, the history of messages is retrieved from the state and passed to the model:**\n",
    "```python\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7abd5d0-5f70-47c5-bdb8-955682833d7b",
   "metadata": {},
   "source": [
    "Let's repeat this exercise with a different thread and see how the `state[messages]` changes with each invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fcf1797-004a-4d50-8929-365039020e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[HumanMessage(content=\"Hi! I'm Jim. I'm from the UK\", additional_kwargs={}, response_metadata={}, id='1aafd88c-bf1c-4cc8-aa7e-c34f81b37a3b')],\n",
       " [HumanMessage(content=\"Hi! I'm Jim. I'm from the UK\", additional_kwargs={}, response_metadata={}, id='1aafd88c-bf1c-4cc8-aa7e-c34f81b37a3b'),\n",
       "  AIMessage(content='Hi Jim! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 16, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b83a7d52ea', 'id': 'chatcmpl-CEU39j4KimKCR4Z9QCil1u0BcPbT4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--54634dae-694f-4eaa-8a9a-b6785343e095-0', usage_metadata={'input_tokens': 16, 'output_tokens': 15, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='Who am I?', additional_kwargs={}, response_metadata={}, id='c10e1e3d-f4a3-46cc-b105-385a4f3256fe')],\n",
       " [HumanMessage(content=\"Hi! I'm Jim. I'm from the UK\", additional_kwargs={}, response_metadata={}, id='1aafd88c-bf1c-4cc8-aa7e-c34f81b37a3b'),\n",
       "  AIMessage(content='Hi Jim! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 16, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b83a7d52ea', 'id': 'chatcmpl-CEU39j4KimKCR4Z9QCil1u0BcPbT4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--54634dae-694f-4eaa-8a9a-b6785343e095-0', usage_metadata={'input_tokens': 16, 'output_tokens': 15, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='Who am I?', additional_kwargs={}, response_metadata={}, id='c10e1e3d-f4a3-46cc-b105-385a4f3256fe'),\n",
       "  AIMessage(content=\"You're Jim from the UK! If you have more specific questions about yourself or anything else you'd like to share or discuss, feel free to let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 43, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CEU3Ac9Z63cmh8Lobvent3E2tve9l', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1adf4307-a9dc-4567-9d1a-684c634f03b2-0', usage_metadata={'input_tokens': 43, 'output_tokens': 31, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='Where am I from?', additional_kwargs={}, response_metadata={}, id='bed26a7b-c414-4c65-948b-dff43d960bc5')]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "MESSAGES_HISTORY = []\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    MESSAGES_HISTORY.append(state['messages'])\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc999\"}}\n",
    "queries = [\"Hi! I'm Jim. I'm from the UK\", \"Who am I?\", \"Where am I from?\"]\n",
    "\n",
    "for query in queries:\n",
    "    input_messages = [HumanMessage(query)]\n",
    "    output = app.invoke({\"messages\": input_messages}, config)\n",
    "\n",
    "MESSAGES_HISTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6453a5a-da2c-4f13-b155-0db7fa8d3abf",
   "metadata": {},
   "source": [
    "We see that every time a new human prompt is passed to the LLM, the previous history of human/ai messages is included. We can also seee the persisted history in the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a153f8a5-c2d8-4331-a585-7d22a3b203e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CheckpointTuple(config={'configurable': {'thread_id': 'abc234', 'checkpoint_ns': '', 'checkpoint_id': '1f08eccc-0a75-6202-8001-c2a854691bde'}}, checkpoint={'v': 4, 'ts': '2025-09-11T05:04:10.514863+00:00', 'id': '1f08eccc-0a75-6202-8001-c2a854691bde', 'channel_versions': {'__start__': '00000000000000000000000000000002.0.8528319022299968', 'messages': '00000000000000000000000000000003.0.48025719766620845', 'branch:to:model': '00000000000000000000000000000003.0.48025719766620845'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.3374717961583966'}, 'model': {'branch:to:model': '00000000000000000000000000000002.0.8528319022299968'}}, 'updated_channels': ['messages'], 'channel_values': {'messages': [HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='bab3ed37-49ab-4405-848b-41b753aac2b8'), AIMessage(content=\"I'm sorry, but I don't have access to personal information about you unless you've shared it with me in this conversation. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 11, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e867127662', 'id': 'chatcmpl-CETlZg95bB6AB4FO7qOFOL8VuvYyq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dd676982-40be-4e15-a705-04b6ae78ed7a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 30, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, parent_config={'configurable': {'thread_id': 'abc234', 'checkpoint_ns': '', 'checkpoint_id': '1f08eccb-f7f4-6934-8000-e88686843562'}}, pending_writes=[])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_tuple(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d1d72-d4dc-467b-a9c3-f704c5909686",
   "metadata": {},
   "source": [
    "### 4.2.1 Prompt Templates\n",
    "\n",
    "As we saw earlier, we can use `PromptTemplate`s to include system directives before the human input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6125686f-fadd-4535-8757-699910edd13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Jim! What be bringin' ye to these here waters today? Speak yer mind, matey!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b1c852a-0521-41a3-8807-d99326556f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yer name be Jim, if I be recallin' correctly, matey! What other treasures of knowledge can I assist ye with?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451be9f-db21-4d37-a7b0-3250d709aeed",
   "metadata": {},
   "source": [
    "As we mentioned earlier, we send the history of messages every time we invoke the LLM. In order to do this with PromptTemplates, we use `MessagesPlaceholder`. \n",
    "\n",
    "```python\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # extract 'messages' from the state and insert them here.\n",
    "    ]\n",
    ")\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state) # state contains history of messages\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd96573-0eeb-4d02-be72-7fb4cf8d591f",
   "metadata": {},
   "source": [
    "### 4.2.2 Prompt Templates With Variables\n",
    "\n",
    "So far our state only contains messages. What if our state also needs to consider other properties e.g. `language`.\n",
    "In this case, we need to extend the state schema to include additional attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e963d9f-ca6a-48e4-8bce-ca7900296b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # state['messages'] will be inserted in this placeholder.\n",
    "    ]\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages] # state stores messages\n",
    "    language: str # messages contain a variable 'language' whose value also needs to be stored with the mesages\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9090221-d2e5-4304-ac11-22c5fd430e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "مرحبًا بوب! كيفك اليوم؟ شو فيك تسأل أو تحكي؟\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Lebanese Arabic\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1aaab4b-e090-4e0c-a3ca-2b80d7055668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "اسمك هو بوب!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0208a60-924e-4a68-a0a2-dd5d4cf122f2",
   "metadata": {},
   "source": [
    "## 4.3 Managing Conversation History"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
