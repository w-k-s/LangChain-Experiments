{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b35517-907c-483a-9902-b7dc781918d1",
   "metadata": {},
   "source": [
    "# 7. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13b139db-ee39-4ba8-8136-ba2a7f440deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a23cf2-b255-4267-a180-cc4159909133",
   "metadata": {},
   "source": [
    "## 7.1 Introduction to RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2d61a-a373-4ab2-a02d-f79318c30cc1",
   "metadata": {},
   "source": [
    "- Retrieval-Augmented Generation (RAG) is a technique where an LLM retrieves relevant external knowledge (e.g., from documents or databases) and incorporates it into its prompt to generate more accurate and grounded responses.\n",
    "- A typical RAG application has two main components:\n",
    "\n",
    "    - **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happens offline_.\n",
    "\n",
    "    - **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    " \n",
    "### 7.1.1 Indexing\n",
    "\n",
    "We have looked at Indexing in an earlier tutorial (Semantic Search). The steps are summarised below:\n",
    "\n",
    "- **Load**: First we need to load our data. This is done with Document Loaders.\n",
    "- **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "- **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "### 7.1.2 Retrieval\n",
    "\n",
    "- **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "- **Generate**: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aeb11c-1fe3-4136-9683-dd347c7acd3b",
   "metadata": {},
   "source": [
    "## 7.2 RAG Over Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17888f4-b43a-4dd0-8f89-b317bd67f14f",
   "metadata": {},
   "source": [
    "### 7.2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1bf7ff99-cd49-4a32-a38a-6f82f7c270bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "assert os.environ[\"LANGSMITH_TRACING\"] is not None\n",
    "assert os.environ[\"LANGSMITH_API_KEY\"] is not None\n",
    "assert os.environ[\"LANGSMITH_PROJECT\"] is not None\n",
    "assert os.environ[\"OPENAI_API_KEY\"] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed397fb8-06e7-43c1-8ad8-d15cd038e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321db6b-791d-43db-a92b-7fa58d639643",
   "metadata": {},
   "source": [
    "## 7.2.2 Load Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ca431-a5e0-4250-96e3-e3b6ad4f2c6d",
   "metadata": {},
   "source": [
    "For this example, we will perform RAG over an HTML document.  We will use\n",
    "- `WebBaseLoader` to load HTML from web URLs\n",
    "- `BeautifulSoup` to parse it to text. In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f2ed0cc-587c-4e10-b205-6ee7473481a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63f799f1-1129-4807-9f05-1410d2f5216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc27a3-1044-4a3f-a3ea-8ca4c9ded1ca",
   "metadata": {},
   "source": [
    "### 7.2.3 Split Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434826f1-3b60-4781-8045-9edcd4e7cee7",
   "metadata": {},
   "source": [
    "Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89349552-e565-4865-8384-ce2eb9b270a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12ce21-3fed-4fd5-90ec-6acc5ff725c8",
   "metadata": {},
   "source": [
    "### 7.2.4 Storing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a634078-451e-40e3-9f61-fc206fe8114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f75382cf-8205-49e6-a123-ef1c6e63d5bf', 'cf6e4ce4-6aec-4de5-a63d-c4b77260aa9b', '213254c1-3115-4a22-9f44-a386194368b9']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84719a79-97ba-4b4a-adca-bd421086da86",
   "metadata": {},
   "source": [
    "### 7.2.5 Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1bf9f-566e-4671-bd69-601a259a0d0d",
   "metadata": {},
   "source": [
    "We will create a simple application that: \n",
    "1. Takes a user question,\n",
    "2. Searches for documents relevant to that question\n",
    "3. passes the retrieved documents and initial question to a model\n",
    "4. Returns an answer.\n",
    "\n",
    "The prompt we'll use is from Lanchain's prompt library [here](https://smith.langchain.com/hub/rlm/rag-prompt).\n",
    "For step 3 and 4, we will use LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7ecdf5e-e299-406a-9a97-c2d7ad2ec7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a585d66-2e1c-4d57-9ed5-490613be85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c18f95-7625-4a58-aa00-695e7ffaf66c",
   "metadata": {},
   "source": [
    "We have 2 steps in our graph: retrieval and generation. We will create 2 notes for these.\n",
    "- The `retrieve` node will search the vector store for matching documents. Matching documents will be set on the state.\n",
    "- The `generate` node willl invoke the llm passing the question, and similar documents.\n",
    "- We store the answer in the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d43a08a-ed42-42de-a92b-d771763d1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22763a-9efc-443c-92ec-27e090baccd1",
   "metadata": {},
   "source": [
    "Let's put the `retrieval` and `generate` steps in the graph and this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e6db699e-91d6-4281-ae28-6683f29486ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9ebfb-318f-4855-88d9-81681b8e56a7",
   "metadata": {},
   "source": [
    "To visualize our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50f554bc-becd-4b6d-9cb8-c8514321d321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76653ad9-7b5a-461e-9492-a9cd012ad394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='acf85735-9889-4fbf-b209-a5914827b279', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(id='213254c1-3115-4a22-9f44-a386194368b9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='fd09199a-2fd0-4ced-8733-4e6341a02253', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17734}, page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.'), Document(id='b7f24bcd-da05-4b07-8b79-777b2f6002c5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17352}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')]\n",
      "\n",
      "\n",
      "Answer: Task decomposition is the process of breaking down a complicated task into smaller, manageable steps. This can be achieved by using simple prompting techniques, task-specific instructions, or human inputs. It enhances understanding and execution of complex tasks by providing a structured approach to problem-solving.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(f\"Context: {result['context']}\\n\\n\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1d844-18ea-4ae7-97fd-0304d03d02b3",
   "metadata": {},
   "source": [
    "#### 7.2.5.1 Using a custom prompt\n",
    "\n",
    "Below is an example of using a prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d52406b-868c-41c1-837e-41a2412dbbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='acf85735-9889-4fbf-b209-a5914827b279', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(id='213254c1-3115-4a22-9f44-a386194368b9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='fd09199a-2fd0-4ced-8733-4e6341a02253', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17734}, page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.'), Document(id='b7f24bcd-da05-4b07-8b79-777b2f6002c5', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17352}, page_content='Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')]\n",
      "\n",
      "\n",
      "Answer: Task Decomposition is the process of breaking down complicated tasks into smaller, manageable steps, enabling better planning and execution. This can be achieved through methods like prompting, using task-specific instructions, or incorporating human inputs. Techniques such as Chain of Thought and Tree of Thoughts further aid in structuring the problem-solving approach for complex tasks. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = custom_rag_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "\n",
    "print(f\"Context: {result['context']}\\n\\n\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284edf2-89f9-4548-9a07-42fa119c6b11",
   "metadata": {},
   "source": [
    "## 7.3 Query Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bfa64-8788-4667-9a48-fd1f7fbe4fe1",
   "metadata": {},
   "source": [
    "- Query Analysis is the process of employing models to transform or construct optimized search queries from raw user input.\n",
    "- For example, this could be transforming user input into an SQL query or a REST API request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf79cf-8b20-41a7-bc98-6d6e5793dff7",
   "metadata": {},
   "source": [
    "We will demonstrate this with a simple exam before moving on to a more complicated one:\n",
    "\n",
    "1. **Load**: We will update the load step to add metadata to each Document split specifying whether the split is from the beginning, middle or end of the original Document.\n",
    "2. **Analyze** We will add a new analyse step where we ask the model to analyse the user's query to identify which section of the original document the user is interested in.\n",
    "3. **Retrieval** In the retrieval step, we will filter to include only splits in the relevant section`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c9fc3-7554-4fbe-8f6c-a4d63fa13acf",
   "metadata": {},
   "source": [
    "#### 7.3.1 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "143e01f3-e23b-44d5-b6b9-b7f52b6329a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 8,\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_documents(all_splits)\n",
    "\n",
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5da7ab-77cb-4a13-90ae-d2aef1fd5b7a",
   "metadata": {},
   "source": [
    "#### 7.3.2 Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f58e3e-69d1-4089-aa3e-3caaddf3de3f",
   "metadata": {},
   "source": [
    "For the analyze step, we will use the user input to populate a data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17cd52f4-2e7e-4d44-8492-a698f5f082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b1583-7568-49b6-aa9e-284ce5aa5aa4",
   "metadata": {},
   "source": [
    "#### 7.3.3 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8312f0b9-eb7e-4edd-a688-91724ab091bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m graph_builder.add_edge(START, \u001b[33m\"\u001b[39m\u001b[33manalyze_query\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m graph = graph_builder.compile()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m display(Image(\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Langchain/.venv/lib/python3.13/site-packages/langchain_core/runnables/graph.py:702\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m    693\u001b[39m     draw_mermaid_png,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    696\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    697\u001b[39m     curve_style=curve_style,\n\u001b[32m    698\u001b[39m     node_colors=node_colors,\n\u001b[32m    699\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    700\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    701\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Langchain/.venv/lib/python3.13/site-packages/langchain_core/runnables/graph_mermaid.py:310\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    304\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    305\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    306\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    307\u001b[39m         )\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    318\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Langchain/.venv/lib/python3.13/site-packages/langchain_core/runnables/graph_mermaid.py:463\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    459\u001b[39m     msg = (\n\u001b[32m    460\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    467\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "289b1e3b-08a8-4369-a3db-f68314501629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='81f05b1d-dbb1-4a0f-b80c-58e0ed0df0ef', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32987, 'section': 'end'}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.'), Document(id='eaa3829f-ab41-4a49-9dc8-3fb5ca57c668', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 37831, 'section': 'end'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully'), Document(id='a7f11226-0eba-485a-9793-ebeec13470ce', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35043, 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'), Document(id='51e55e79-6146-401e-bd1d-af0840f1ada3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 33804, 'section': 'end'}, page_content='FILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'The end of the post emphasizes the importance of thoroughly detailing each aspect of the architecture before translating it into code. It instructs to methodically outline the core components and then to meticulously implement them in a structured format with properly named files and appropriate comments. Finally, it stresses the need to ensure all parts of the implementation are complete and compatible without leaving any placeholders.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693a443-b6b5-4c41-9677-270589995a8e",
   "metadata": {},
   "source": [
    "### 7.4 Chatbot RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f7bc1-21d5-400c-a0fc-c5a1a5e15ea5",
   "metadata": {},
   "source": [
    "#### 7.4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93564d4d-cf92-4160-a860-934bbba9c0ae",
   "metadata": {},
   "source": [
    "In this section, we will focus on adding logic for incorporating historical messages. We will cover two approaches:\n",
    "\n",
    "1. **Chains**: Deterministic flow\n",
    "2. **Agents**: Dynamic, reasoning-based flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9633aa2-026f-4cc5-8fcf-d6a8546e1bcd",
   "metadata": {},
   "source": [
    "#### 7.3.2 Retrieval Tool\n",
    "\n",
    "Earlier we saw that LangGraph could automated storing message history with:\n",
    "```python\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "```\n",
    "\n",
    "- User input is represented as `HumanMessage`\n",
    "- The LLM response is represented as an `AIMessage`\n",
    "\n",
    "We can extend this approach for RAG as follows:\n",
    "\n",
    "1. User input as a `HumanMessage` (same as before);\n",
    "2. Vector store query as an `AIMessage` with tool calls;\n",
    "3. Retrieved documents as a `ToolMessage`;\n",
    "4. Final response as a `AIMessage` (as before)\n",
    "\n",
    "**In simpler words, rather than providing the documents in the state as we did in the previous section, we provide the LLM with a tool to retrieve the relevant documents when necessary.**\n",
    "\n",
    "This has the benefit that: \n",
    "\n",
    "1. The LLM does not invoke the tool when a direct response is sufficient e.g. in response to a generic greeting from the user).\n",
    "\n",
    "2. More importantly, **Leveraging tool-calling to interact with a retrieval step (step 3) has the benefit that the query for the retrieval is generated by our model.**\n",
    "\n",
    "    This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:\n",
    "\n",
    "        Human: \"What is Task Decomposition?\"\n",
    "\n",
    "        AI: \"Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model.\"\n",
    "\n",
    "        Human: \"What are common ways of doing it?\"\n",
    "\n",
    "    In this scenario, a model could generate a query such as \"common approaches to task decomposition\". Tool-calling facilitates this naturally. \n",
    "\n",
    "    **As in the query analysis section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7c529-0675-4002-88c5-447ef5430757",
   "metadata": {},
   "source": [
    "Let's start by building a custom tool that can perform the retrieval. See [this](https://python.langchain.com/docs/how_to/custom_tools/) guide for more detail on creating tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6cb5f65d-b7c0-4108-b654-3ac47322b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be24f2-3320-4621-9e23-0351442477d0",
   "metadata": {},
   "source": [
    "#### 7.4.2 Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403835e3-85e1-435f-9a54-527ccbddb13b",
   "metadata": {},
   "source": [
    "A chain is just a sequence of deterministic steps that run one after another, like a pipeline:\n",
    "\n",
    "![](./docs/rag-chatbot-graph.png)\n",
    "\n",
    "The `query_or_respond` node decides whether a tool call is necessary or not.\n",
    "- If a tool call is necessary, call the retrieval tool and generate a reponse.\n",
    "- If a tool call is not necessary (e.g. the user just said 'Hi'), respond directly.\n",
    "\n",
    "**When using Chains**, at most one tool call will be made and the output of the tool call will be passed to the generate node. While this is **predictable**, this does not leverage the reasoning capabilities of the LLM. **The Chatbot will not be able to perform Chain-of-Thought prompts.***\n",
    "\n",
    "The implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca684621-63ae-4012-b0ee-227044aeac37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6f20b-fca5-4114-a32d-51e0dd240fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4cf33-2e09-442f-a482-1fb18e6e78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Hello\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97535069-9761-471c-ad1d-0f2805538970",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7766f3-cd6f-41d0-acad-bc44a7a0ad5d",
   "metadata": {},
   "source": [
    "#### 7.4.3 Chat History "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12eb52-b714-4b8b-b631-65374bf0d26a",
   "metadata": {},
   "source": [
    "In production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.\n",
    "\n",
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "To manage multiple conversational turns and threads, all we have to do is specify a checkpointer when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).\n",
    "\n",
    "For a detailed walkthrough of how to manage message history, head to the How to add message history (memory) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2cd6d4d0-b211-4aa2-9ff9-58da268ac5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Task Decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_xSv7WueJnxQPD1LngaP3MAPJ)\n",
      " Call ID: call_xSv7WueJnxQPD1LngaP3MAPJ\n",
      "  Args:\n",
      "    query: Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578, 'section': 'beginning'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638, 'section': 'beginning'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complicated task into smaller, more manageable steps or subgoals. This can be achieved through various methods, including simple prompting with language models, task-specific instructions, or human inputs. Techniques like Chain of Thought (CoT) and Tree of Thoughts further enhance this by encouraging step-by-step reasoning and exploring multiple possibilities at each step.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cfd7789b-757f-4de5-899f-927dd2cb7629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you look up some common ways of doing it?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_lS94VqYHnuFYnpTY5f2z8FqS)\n",
      " Call ID: call_lS94VqYHnuFYnpTY5f2z8FqS\n",
      "  Args:\n",
      "    query: common methods of task decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578, 'section': 'beginning'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638, 'section': 'beginning'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Common ways of task decomposition include using simple prompting techniques like asking an LLM for steps or subgoals, providing task-specific instructions, and gathering human inputs. Additionally, methods like Chain of Thought (CoT) encourage step-by-step reasoning, while Tree of Thoughts explores multiple reasoning possibilities at each step. Utilizing external classical planners via PDDL is another distinct approach for long-horizon planning.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Can you look up some common ways of doing it?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44db419-34e1-41e7-b8c9-44d7b05dc89a",
   "metadata": {},
   "source": [
    "#### 7.4.4 Agents\n",
    "\n",
    "Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above \"chain\", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "63ffdfad-7062-4e74-9912-6ab3203edac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAQAElEQVR4nOydCXwTRf//ZzdJk973SVvaUihQjoIFFBUQEPThVhS55HgQBEH8C6jPDxTERwERFRU5BOSmyk0BueQSylV4OFqkylF60IPeTdrm2v1/N9umaUmKhWY7m8z7xSvszsxuk80nM/P9zsx3pCzLIgKhsZEiAgEDiBAJWECESMACIkQCFhAhErCACJGABUSItclJ0ySfLS4t0FUo9VqdXq8xpFIIgZuLZhFDcWcSltUbDmjEMpBuyGcQSyGaMqQYswwHDMtSLFeeliBGb5JruKGxZGU6y/0tOECoKh2SuPtWv0laxjJayngqc6RlMlrhQgdFOD7VywOJEIr4EXkybqpP7sotylNzX7qEcnaXyWQUPB6dmqkuBOIwnNFSmtFxRxRNsQxrEI3hgIJLQD0sqiFEChIow3OGOzP66lwasgyX1xAi4v4KJeGUx9+KUzHc2fSNyGhGW30uU0hA39oKvbqc0epYuYIODFP0fysQiQciRJR7Txu/OlNdrnf3cWj3rFvb592RqGHQ8W15d5KVIEr/EPmr7zZBYsDehfjL15kPMsubtnIdMMEf2RYF9/XxazPKSnQvvObfspMLwhu7FuLK/9xxcKDHfRqGbJfkhNI/dueGtHTpNx7rX5r9CnHt3NTAMMeXx9laRWiWtXNSn+rj2b4bvr0OOxXiig9vN49x6zXcF9kNq2en+oUoBr4dgLCERvYH1IVhrVztSoXAhM/DctLLTu/KQ1hid0LcuzILXCQvjfVD9seETyOunilGWGJnQtSjtL9U4+aGIbuEkqLQ5k7r5t9D+GFfQtywMN0v2BHZMQMmBYJDJ+WSEmGGfQmxJF/9ukgcvNYDhgET4rHrKdqREON/ynJ2kyEJEpKPPvpoz549qP68+OKLmZmZyAr0mxCkKtEhzLAjIWanVoS2dELCcuPGDVR/srKyCgsLkXWQOSC5QnJs6wOEE3YkRE0F89QLXsg6nDlzZtKkSc8999zgwYPnzp2bl8e1fbGxsffv3//ss8969OgBp0qlcsWKFWPGjOGLffPNNxUVFfzlvXr12rp161tvvQWXnDx5csCAAZA4aNCgGTNmICvg4eeQebcM4YS9CPH2tXJagjz8rdIw37x5c/r06Z06ddq+ffsHH3zw119/zZs3DxnUCa8ff/zxiRMn4CAuLm7dunWjR4/+9ttvofyRI0dWrVrF30Emk+3atSsqKmrZsmXPPvssFIBEaNOXLFmCrIBfqKJCxSCcsJf5iFl3yyRSClmHK1euKBSK8ePH0zQdEBDQunXrW7duPVxs1KhRUPOFh4fzp1evXk1ISHj33XfhmKIod3f3mTNnIkEICJXfOEeE2BiUK/U0bS0hxsTEQCP73nvvdenSpVu3biEhIdDCPlwMqr2zZ89Cww1Vpk7HmQteXtVdBZAvEgovXwd+piM+2EvTzHBj6tZ69C1btvzuu+98fX2///77IUOGTJkyBWq7h4tBLrTFUGD37t2JiYnjxo0zzXVwcECCIZUYJp1jhL0IUeEk4efoW4muXbtCXzA+Ph56h8XFxVA78nWeEfgh7NixY9iwYSBEaL4hpbS0FDUSxbkV3JRvnLAXIQaEOPJz9K3BpUuXoLcHB1Ap9u/fH0xdEBm4YEzLaLXa8vJyP7/KMW6NRnPq1CnUSOSkq2nMOmX2IsSoTs56Hasps4oWoSEGY3nnzp3g/EtKSgLrGBQZGBgol8tBeefOnYOGGOyYsLCwvXv3ZmRkFBUVzZ8/H3qWJSUlKpXq4RtCSXgFsxruhqxA1t1yB4Wwnv1HYUd+RJkDff5QAbICYA5Dg/vVV1/BcMjEiROdnZ2hLyiVcnUOmNIXL16EOhKqwy+++AKM66FDh4ITsXPnzlOnToXT3r17g6+x1g2Dg4PBlQhOR+hWIiuQn6X2byJHOGFHE2PjFqerSnX/nh+O7J7v/9/fE+Y3c3TFqBqyoxqxz2j/slLsxliFZ//aLJmcxkqFyK4W2HsFODi5yPauvD9wUpDZAnq9HhzOZrPAtuDWOZuzNCMiItauXYuswzoDZrNcXFxgzNBsVnR0NIzQIAukJqs69vREmGFfa1YyblXsWpYx7ZtISwUe7q7xwFcOX7zZLOgLGm3hBqfUgNkscKFDF9NsFvxmwFoym3VoY+7dpNK3FzVDmGF3i6c2L0wDP87o2U2RXbJsxq0hk0ODIgV0nv8z7G7NysiPQmG478LBImR/rJ2bGhzphKEKkX2u4pu4IOLi0fzSXPtqCrZ8mQE2yqDJQQhL7HeB/Y8zb/d6IyAq1hnZAes/S/MOcuj/b0wXNSM7Dzny44zbQRGOg9/BtJJoKNZ8kqpwoqFPgjDG3oMwQbdJU6Hv8rJPhx4iDwJmjp3fZ95PLW/Rwa3PKNzXcZOwdOjMnvxrZ4ooGoU0d+o7OlCKY1e+fty6oko8WlCQrXFylYydE4bwGlU2DxFiJSd2PPgrsVRTwVASytFF4uohBe+3RMZoNdXPRyKl9LrKU8M0W5YxCbAJ50xl9FiKm/XF8ulcAE+aCxpbGQeWZRDF5cPdaD1cwIXr5C6kaEMqxcemNRSjKUOYWMN3xAeihTJwNyn3ylRdiDjHIa3XobISnbJUV6HSQ0l3H1n3V32Dm4tmETcRYm1O78nP+LuMi1us5Z6NUXmoUgaVx1yUWBZVPzxDUNdKkRnUw0/DrRQXVVkSDvR6hosrS9FcGGO4HVuVWxWumMdQ3hDWGPG3M9yID6BsiCprUCrLGkIpy+SUhKYcHGlXL1mLDq74R0N8GCJEoZk2bdqIESOeeeYZRDCBBHMXGp1Ox88QI5hCnojQECGahTwRoSFCNAt5IkKj1WplMhki1IQIUWhIjWgW8kSEhgjRLOSJCA0RolnIExEaECLpIz4MEaLQkBrRLOSJCA0RolnIExEaIkSzkCciNESIZiFPRGjAoU2E+DDkiQgKy7IMw0gkYpiqKixEiIJC2mVLkIciKESIliAPRVDIjAdLECEKCqkRLUEeiqAQIVqCPBRBIUK0BHkogkKEaAnyUASFGCuWIEIUFFIjWoI8FKGxFMvVziFCFBQY3MvOzkaEhyBCFBRol2ttjUbgIUIUFCJESxAhCgoRoiWIEAWFCNESRIiCQoRoCSJEQSFCtAQRoqAQIVqCCFFQiBAtQYQoKCBEvV6PCA9hjztPNS4wuEK0+DBEiEJDWmezECEKDRGiWUgfUWiIEM1ChCg0RIhmIUIUGiJEsxAhCg0RolnIzlMCERMTQ9OVpiE8cziG1/79+8+fPx8RiNUsGO3atUPcNnoc4EqkKCowMHDUqFGIYIAIUSDefPNNZ2dn05T27du3aNECEQwQIQpE7969TWXn7e09fPhwRKiCCFE4xo4d6+bmxh+3bNmybdu2iFAFEaJwPP/881FRUXDg7u4+cuRIRDCBWM0PoUen9haqSjQ6TY2pCRTYuQxDSyhGb/LEqrYE5w1ifiN6lmFN87kNxbl9vrnEoqKipOQkF2fnmJgO/DbhLFN9s8qt7Lmj6vSq/e0r97Tn7wOncGOGqfHdOThKA0Ic23d3RSKECLEG277OfJBVIZNL4PvWa2s9GW4jeYquIZ2q3eWRUVUsxVCsSTtD8bvYVycyLENTFH8rzpHDUtVl+ZtThiuq0o2JyFT/hk3sUY13ghwUlF7HXdlrWEBkByckKohDu5o9K++ripnRc5ohMXP7ivJoXA7t4B8RLSYtkhqxkp1L75cp9YOmhiCbYNPnd0bNinAVT3QTYqxUkp1R0WtkMLIVfAIU8WvSkXggQuRI+qNUIkUunhSyFQIjnFQlYhrRJn1EDmiUGS2yJRTOlFYjpgUJRIgcOkanZ2yqrww9f4ZBIoIIkYAFRIgELCBC5KAo2zFTqhHVZyJC5LA9ZyoLQzyi+kxEiByszVWIMEwort8WESIHbaONs4ggQuTg5h7YWONMkT6iCLHB+pCtMVsHf4gQOWzPWGHE9omIEDlomraxOlF0n4cIkYNhRFeD2BpEiAQsINPAOCi8rZVdu39dsGgusmlIjciBufcmJeUGsnWIEDkeo0JUKpXbtm+6cPFsauptby+frl27jx83WaFQIEOPc+l3i06fOeEgc+jV66U20e3/M/u9HdsOeXl563S6NWt/PHf+dG5udps2MUMGvf7008/xNxz8Su9xY98uLi5av2GVo6Njp9hnpr4z09vb5733J169ehkKHD68P37PCRcXF2SLkKb5Mdm5K27L1nXDXh/9xeffTpo0/cTJIyAgPmvb9s3x+3ZOmzprxYpNjo5OoDxkMMzh9bvvv9y+Y8uQwcO2bI7v3q3X3E8/OHnqd/4qmUz2yy8boNjuXb+v/3nH9aQr69avhPRvv17VqlWbPn36Hf89sR4qJA5tMfIYLfPrr40CJTVtGs6fJiVdvXAxYdLEd+H40OF93Z7v2aN7bzgeOWIcpPNl1Go1ZI0YPnbggFfh9F8vD4KrNmz8Ce7DF2jSJGTUyPHckYsr1Ih//fUnemyIQ1uMUFS9WwaowC4mnl24aO6t23/x8Q49Pb3gVa/Xp6beefmlgcaS3Z7vde3a/+AAhKXRaEBhxqyY9k/9dnBvcUmxu5s7nLZo0cqY5erqplIpkd1AhMjBsvWeVr/qp+8PHNgNjTIIy98/YPWaZQd+2wPpSpUS6lcnp+rAX+7uHvyBUlkKr9Om/7vWrQoL8nkh2vPMCyJEDqqe/huQWvy+HUNfHdG/3xA+hRcZ4OTILWvXaqvXYhUW5vMH3j7cMuMZ78+GJtj0bn5+Aaih4UJOiKqTSITIw9arRwXtb3l5uY+PH38KDW7C2VP8MTTZfn7+YEobC59JOMkfBDcJlcvlcNAhJpZPKSwsMFSfDR+SAYb4xDUzlljNHGz9dMjFwQ4NDYPuXeb9DHC4fPnV/LZtYkpLS1QqFeR2fabb4SP7LyaeA5GBBQ3p/FUguLFjJoF1cv36FdAu2MszP5jy7dKFj/xzUIP++WfS5f9dNK1oH/WRRGasECFyUPUfWvl49hcKuWLsuKGj3hz8VMfOEyZMhdMhr/bOyr4/5s2Jbdt2+ODDqaPfHHLv3l1owRGnXRm8vjHszVkzP9kSt27AoB7gawwKDJ4xY84j/9aAfq/A+5v1wTtlZSpko5DYNxwJ+/Mu/148Zm7DhF+qqKgAfzVUmfxp3C8bNm9eG7/3BBKQm+eLzx98MPXrSCQSSI3IwU0Da7gnAcqb+PbIHTvjoNU+dvzwr9s2DRw4FBHqhBgrHNw0sIaLizB2zMTi4sLDh/f9tPp7X19/GEcBtzYSGDKyIkYafPbN9Hc/RI0LGVkRI4bZN6Sv3JgQIXKA142ibWpUQ3QmKBEiB8OyrKhiZz0S0Q0WEiFyUJhP0bYDiBA5bHCBvdggQuQgNWKjQ4TIwYqwd183XDQw4kcUHZTNNcyU2D4TEaIB0jQ3NkSIHMRYaXSIEDkcHKQyhY0Fv0EymQSJBzL7hiO4mRMjKID27QAAEABJREFUpt1xHk1RllZcPy0iRI6ACAcHB/ribwXIVsi4rQyKENOmkESIlbw0JijlchGyCQ6uzWIZ9qUxfkg8kBnalZSXl78/fXZb93e8AxRhLd3kzqzOdEtvCjEPbRdu3EKZYqvDwdO1d1HmYGlEmaRWX0jxi/u5i1lzEwir/k51rvFWxj9qLCOlJflZmrSUEoWzZPgskW1wSYRYycaNG6Ojozu26Ri3NL20QKfRMYyu+slIpEhv0omkDMIweXBV24dXndSC34W++nKqcl9wQ3r1tcYLKZrbatz0zsZs4zsx6s9YRianZDKpVpLT9kVt8+bN/fxIjSgeCgoKli5d+umnnyKhmD59+rBhw7p27YqswJo1a1at4mI4ubq6urm5hYaGtm/fvkWLFh07dkR4Y+/umzlz5oAykID4+Pg4Ozsj6zBy5Mj9+/enpaUplcrMzMybN28eOXLEw8MD/uKePXsQxthpjZidnX3+/PlBgwYhm2PFihWrV6+ulQjf8qVLlxDG2KPVXFxcPGHChKeffho1BvAbUKvVyGoMHTq0SZMmpilyuRxzFSJ7E2JWVhY0WDqdbt++ff7+/qgx+PDDD2/duoWsBjT9zz33nLGhg4MFCxYg7LEjIV69enXixInwPXl7e6PGA34A1gh2Y8rw4cN9fbmAT3yLvHv37uXLlyO8sQsh5uTkIEOczPj4eD4MUiPy5ZdfhoeHI2sSHBwcGxvLMExAABdn7Ouvv4aBo2nTpiGMsX1jBazFY8eOgY8G4QH0DaBSlEqt7q/o06fP4cOHjadnz56dPXv2hg0bQKYIP2y5Riwp4cJwlZWV4aNCYPLkybm5ucj6mKoQeOaZZ6CNnjp16qFDhxB+2KwQ165de+DAAWToMCGcgOYSHM6oMQAXN2jx1KlT33zzDcIMG2yatVrtgwcP4IlPmTIFEcyxZcsW6K487G5sRGxNiPBwoW8EtQ50zxGWwLAH9NL43S4aEfAhvP322+vXr4cBQIQBNtU0b9++HXyEMMCKrQqBUaNGVVRUoMYGxqChjZ43bx40HQgDbESI27Ztg9eePXvCrxzhTVBQECa/E5lMBm10UlLS559/jhobWxDijBkz+A6Gl5cXwp64uDgBfDf/nDlz5rRu3XrkyJH8bjGNhbj7iImJieC5Bc9crdFVnLl3717Tpk0RZqSkpIwZM2blypXQZKPGQKw1okajgdF9vssvIhVC7xDqHoQfUVFR586d++6777Zu3YoaA1EKsaCgIC8vb8mSJfjP96wFtD8REREIV9asWXP//n1orJHgiKxpBv299dZb4Kz29PREBOtw8ODBVatWgWfH1dUVCYXIhLhz585OnTqFhIQgcaLX67OysvAc7TUFnJ3QZVy4cGGXLl2QIIijab5z584777wDB6+88op4VQjAkA/+DiYAfLHHjx/fsGEDND5IEMQhRBgv+eSTT5D4oSgKQ5PZEsuWLVOr1eAdQ9YH66Y5OTn52rVruM1asDdOnjy5YMECqB2tuj4V3xoRTOPFixf3798f2RDgdQKzFImK7t27b9q0aezYsdevX0dWA18hwvDDunXrhDTcBKC8vHzu3LmiG0Tw8fE5cOAAeBn5ue7WAFMhbt68+cKFC8jmcHd3//HHH+Pj4xlGfNtpXLlyxXorzjBdYJ+bm2urMVxlMtnAgQPT09NhWEhEY0J///13ZKQV9zrFVIhgoGA1M6DBASfUoEGDtmzZYr2oDw0LCLF58+bIamDaNAcEBEC/BNk0e/bsSUlJUSqVSAzcvn3bqjUipkLctWvX3r17ka0DY+WZmZkJCQkIe6zdNGMqRBhThqEwZAdERUXFxcXhXy/eunXLqkLE1KENQ2FgVzZWVBDhAecifF5sx6CLi4thcPX3339HVgPTGtHX19d+VIgM6wcKCwsbay7gI7F2dYiwFeKhQ4d++eUXZE+0bdsW6kXweCP8sF8h5ufni24o7MnhF99cvnwZYYa1fTcIWyH27dv3jTfeQPaHk5OTQqH44osvEE5AjWhtIWLqNG7cyHGNS+vWrW/evIlwwn6b5pMnT65fvx7ZK2CiwismnlQYjQTb0drh/DAVIvgL0tLSkH0D5svMmTNRYyNABxFh2zR369ZNdCv0Gpzw8PCxY8eixkaAdhlhWyN6eHjgv8JIANq0aQOvjRtFzq6FeOHCBfzDPgsG1IuNuORKmKYZUyHC2Ovdu3cRwYCnp+fixYvhwBie5qWXXhowYACyPmq1Ojc3V4CVk5gKMTY2ll8/SuDhl0yAx1ulUvXv3z8vLw+GBAUIQiyAB5EHUyG6ubmJaNmlYCxduvTll1/Ozs5GhuUvVp2FwGPt2V9GMBVicnLykiVLEKEmw4YNKysr448pikpJSeFFaT2EsVQQtkKEx23V7ZnEyIgRI27fvm2akpOTA55/ZE2EsVQQtkKEYa5Zs2Yhggn8hEWJRGJM0Wg0R44cQdbE2isEjGDq0HZ2dsY5fFujEBcXd/ny5YsXL54/fx68CllZWf7OHdkSr6M7UwKCAhGLaAox/Cxnkw3MTfcjhxNu33v+sGrzcsqwAzlfrvK46iplSWmYT/eMZCqdKqm+kenu6FCP1VwWS9XcN52mKb9guU+TR4dqxmuG9oQJE+ARw1uCprmkpATcFlANwPHRo0cRwYSf598pK9ZTNLebPWWQBmv41hmGpTit8YqjWENX0rDVPSc7g+4q9UdTLFN5QDGGJIMauf9ZQzFkKG8UR/WxqZoNr6YCMkqZRyqDv0fJHKh2z3p2+ZcHsgxeNSK0yJs2bTJu/QCuCmSYrY0IJqz66I5PqOPQyYEI370TapCcUHz9TEFgmDy0tcWdjvDqI44aNerhkb3OnTsjQhWr/u9Oq07eL44UjQqB6K7uw2aFH1iflXi42FIZvITo5+fXr18/0xRvb288g043Cr+tz5XKJDG93ZEIadXF48rJfEu52FnNw4cPN60UY2JiMNkaCQdy0ip8AhVInHTs5aXVshoL62axEyKMqcAoKh9vxMvLa/To0YhQhVatkypEvDUOw6C8HPOrw3D8VMZKsY0BRKhCp2F1Gi0SLaye1TPmY2s9kdWsLkdn9z/ISVVXlOs0FSzondFX2+60FDEmexlxhj3/WuV5omm4pMbbAn8E5EKZHk0X6IP1Uol0+Yd3TC8xvZXhk9VIpyXwBtDDQPVK0TS4Epxc6eAWTl372++CmMaFEwBj3l34mEI8uD4n7aZKq2ZpKS2R0DJHqcxJArJiTb2nVA0nZaUjysQ3VauAsRC8mFiE5kuh2iI08JB/lUcqlcA706v1Bbna3IzCS78Xyh3pVp3dnx9MFCkohhqkgWrE337OuZuspCWUq69Lk2hRfpF6DZOe9ODa6aLrZ4o6vuD59L9EsIOfjcAN3jREjbjyw7twm9C2AS5+YrXdAIkDHdaRi2eSe7vk0rGCG+dLx39KppwJguVRvH9qrKTdLP/h/Vuufs4te4SKWoWm+DVzi+4VRtGSH2feRmKAG6+jxRxIF2wAiraQ8w8oeqDduyqzdc/woNY22KkK7xwY0MJ3mSi0SLGWOvuigBv0thA8/NFCvH21bMuXaW1eDKclyFbxCnGO6ByybAbuMyDBeyBiGfLGioXQ6I8W4sGNWS06N0W2jqOrxCfMc/kH4mijRQo/Dchs1iOE+NOcVLCOpS62GeC/Fv6RHhIHyeZF6YhgHaq8c2aoS4gnt+fpNExoOxsPqm5Ki2dDCrLVWXc1CEtosFTEvOsHaxizMEtdQkw+XwStFbIznL0U8asyEJYYRgxE3EukUO1BMiMWhXh6dz4Ml/mGuyEsuXL96MyPuyhVhaihiYgNVKuZ4jw9wg+KQsLvgzT4ld4bNq5GDQFXI6J69hH/vlLq6u2E7BIHueTwJusu03w8WK5CrF+N+On8jw78tgfhAfcjsvA7sihEVanOr5mdDsW6+LrmZ6uRTZCScgNhA/cjsvA7Mj/Ed/OCEoTr6G6tFS2padcOH1+dnnHDxdmzVdRzfV6YoFBwO4GdObftyMm1k8cv3xD3n5zcO4H+kd26Du/UsXKn3H0Hv0+8ekDu4NShXV8/n1BkNQKauRdmFCEMgYEVVI+m+YVesfC6+KvPlq/4Jn7PCTg+c+bk+g2r7qXddXf3iIyMmj7tQ3//AL5wHVk8UBnv2Ln10KF96Rn3moaGx8Y+PX7cZNPlrf8IC8XN14h3kpS01Fr+67z89JXrpmm16qkTV48ZsSgr5+/layfr9dyMMYlUVl5eunv/V68P/r/F88+1a9Pz193/LSziWsmECzsSLmx/pd+s6ZN+9vYMOnJ8DbIaMBgN9mlKogrhBovq5dI+eOAMvM6a+TGvwsRL5z+ZN6tPn36/xh2Y+/HCnJysb79byJesI8vIzp1xmzavHfrqiLgt+wYMeHX/gd1xv2xA9cVC39u8EFXFOqnMWnNmL189KJXIxg5f5O8bFuAX8dqg2ZlZKUl/VkYs0Ou1L74woWlIW+iVx8b0g19hZtZfkH767K/tonuBNJ2c3KCOjIyIRdaEklC5GRXItlj78/Juz/cEJUGdFx3dbsrk98+dO33T0HbXkWXk6rXLUVGt+/bt7+Hh2b/fkGU/rOvS+VlUH+qozM2rTatjWWQt6wza5ZDg1s7OlatcvTwDvb2C7967YiwQ2iSaP3By5Gz28opSkGNeQbq/X7ixTHBQS2RNwDwtV+E3F/rJvpM7d/5u2TLaeBrVojW83ryZXHeWkTZt2l+6dP7LxfMPHoovLiluEhQcGVnv5USshY9gvhdoDBlgDcorlOmZN8D5YppYUlq9vuthD0WFWsUwerm82op3cHBEVgV6YxgOrj/Bt6JUKtVqtVxePXPKyYl7nmVlqjqyTO8A9aWTk/OZhJOLvvxUKpX26PHipLfe9fGpx6pz1rBy3yzmhShzkNDIWo40V1fv8KYxfXtONE10dq5riaRC7gyy0Gqr20q1pgxZE5ZhFY7YLeh5kpEVhYLTWUVF9dollUFn3l4+dWTV/Os0tMjwLzX1zuXLF9ZtWKVSKb/4bz3CKlPIYqVuXogevvL8LGvtlxnk3/zS1QMRYR2MER2yc+/4etdlBUMd6ekRmJp2vXtVn+TPlDPImsAYRkC4lSvd+vMkIytQh0W1aJWcfM2Ywh9HNGteR5bpHcBebtGiVXh4s7CwCPhXqizdf2AXqg9sff2Izdo663UWxmKeGPDIMAyz97dvNJqK3Af39h36YckPI7JyHjEFq32b3tdvHIcBFTg+9seGexlJyGpolHqwTiPbi96fL5fLfX39EhPP/e9Kok6nGzJ42OkzJ3bs2FpSWgIpPy7/umOHTs0jo6BkHVlGfj92ECzrhIRT0EEEU+aP08faRLdH9YI1v6gIWaoRw9s6MSxb+qDC1bfhJ2OD2Ttz6pbjf2z8dsWY3AepocHRrw2e/Ujjo3f3cSpV4e4DSzb9Ohta9oEvv7dl2ydWiiCVm1ooU/VQZMgAAARQSURBVNjI7MuRI8b/vG7FhYsJW7fsA+/Mg7zcX7Zt/OHHJeAjjH3q6bcmTOWL1ZFlZMb7c35Y9tXsj99H3JJzb2ijXxs6CtWHOppmi9HAfp6XyiBJsy5ByP5IOZkW0FQxaHIgwozlH9xuEun4wjCxfinr590aPLlJcAszfR6L/fEOPbzUKkxnQ1kbjVo36G3sVAiACUlJRBzpgUX1HOIDYnq4nfstL+tmYWBL8zPBiopzvvphhNksR7lLudq8rRPgGzF14k+o4ZjzeS9LWTBaI5GY+YBhoe0mjLZo6906n+Xm5WA1L+oToYdxFb21+u4CYIjHaD6rrtHkTn29zv+Wb0mIri7e70/ZaDYLrBAHB/OdS5pu4PFrS++BextatYPMzFaGUkldEd0qSirGLxQiWO9jQFF4/kDqAVvfGhF4qqfHtT+K7yZmhceaaaegsvHybPzOSsO+h5RT6U0inWhcQw+yrLgXT9Xh0H5Eh2Pc3KblJeqiLOt6jzEh/foDiYQdMgVjU4CygTrRPI/u+U5Z1CwjORfZOtk3ClT5ZRP+G45whhV3nQgjQ+xjrFkxFpm8qFnykbsFmfhNi2ogMq7nF+cp316E+z4GYl44xcHFmq/vmhVTwPR85+vI+3/m3rmYhWyOvxMyK0pUkxbgXRcaEPPCqUdQD6fU1CWRiNHdOJaalVKAbIJ7Vx8kHbnr5S3FvUW2A+rnTBk/L+z8ocIrJwqL7pcqXOW+zbxcPMUT3L6Kgkxlwb0SdZla7iR9ZXJIUHM5EgkUTVFiDsLEhRyhzdd99fbqdenrCf8SjxYlnS1OvZQJZpyE8/cjGl7p6lWr/GYx/PJBFlG1NiTiVl4wxi2QqlucWtvFVF5r3A6pZkmEKrdMoir/kHEXmuq/yF9CS6CERK/VM4ye0XHRV9x95L3faBIWjd38mrphGZa10SBMj+leju3tAf/g4Nb/VHeSlIW5GnUFA99xtRANujfEIWYNuqjc8KgyV8rNwuALwy+Ee28G4UiklF7HPWiqSk1wmTEgMRfYGHEXGlclcnegDXFimcqwx4jioyOz/G35V6mMksopqVTqHegU9ZRbk+Y2ElbPlnjScY7IDs7wDxEITwamm0ISzCJzkEhlIp6fJpVyfTjzWYggHmQKSl0m4kkP0NsKjjBv3Yp4TpEdEtZKxCEoEvbmyR0l9VtgT8CT7q96gZF2bIsoR1zvJZf0fM3PUi5e+zUT/gkb/ptG03RMD5+m0SIw/5VF7OWjD+7dLB0zJ8zZ3WIHlwhRlGz7NrMgW6PXMXp9za+P/UeL8ClzE6X5/cbRP8BsScPO5LUTaQnngHd0kfYZ6R8UWdfPhghRzGhQebnJ8nOKX7BZ5f1HtRz9vG+2crN7xJiOPSBkDO1hLFx5uclwQmUxkyz+gspLqvb9Yo1vhpu16uiC/glEiAQsIO4bAhYQIRKwgAiRgAVEiAQsIEIkYAERIgEL/j8AAAD//yyHBq4AAAAGSURBVAMA6/m8ikgUboIAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n",
    "display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91371d8f-8f50-4d58-981d-4dddb68f13ad",
   "metadata": {},
   "source": [
    "The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. **The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information. i.e. the LLM can perform chain of thought** \n",
    "\n",
    "Note that the agent:\n",
    "\n",
    "    Generates a query to search for a standard method for task decomposition;\n",
    "    Receiving the answer, generates a second query to search for common extensions of it;\n",
    "    Having received all necessary context, answers the question.\n",
    "\n",
    "We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "627c5097-c2ad-4f5a-878a-cc0d16cf5338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_fkjCygMnvFM3UwejkcFtaU48)\n",
      " Call ID: call_fkjCygMnvFM3UwejkcFtaU48\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578, 'section': 'beginning'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638, 'section': 'beginning'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_ecgiu7IVjSQUffa6nKdDL0wf)\n",
      " Call ID: call_ecgiu7IVjSQUffa6nKdDL0wf\n",
      "  Args:\n",
      "    query: common extensions of Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578, 'section': 'beginning'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638, 'section': 'beginning'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition involves several approaches:\n",
      "\n",
      "1. **Prompting**: Using simple prompts to guide a language model (LLM) by asking questions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\".\n",
      "2. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as \"Write a story outline\" for creative writing.\n",
      "3. **Human Inputs**: Involving human inputs to assist in breaking down tasks.\n",
      "\n",
      "In addition, there are some extensions of the task decomposition method that include:\n",
      "\n",
      "1. **Chain of Thought (CoT)**: This recent technique encourages the model to \"think step by step,\" enhancing its performance on complex tasks by breaking down larger tasks into more manageable components.\n",
      "2. **Tree of Thoughts**: This concept extends CoT by allowing the model to explore multiple reasoning possibilities at each step. It creates a tree structure, decomposing problems into numerous thought steps and generating various thoughts per step. The evaluation can utilize search processes such as breadth-first search (BFS) or depth-first search (DFS).\n",
      "3. **LLM+P**: This method involves using an external classical planner for long-horizon planning, employing the Planning Domain Definition Language (PDDL) to describe the planning problem and manage the decomposition process.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
