{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b35517-907c-483a-9902-b7dc781918d1",
   "metadata": {},
   "source": [
    "# 7. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b139db-ee39-4ba8-8136-ba2a7f440deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a23cf2-b255-4267-a180-cc4159909133",
   "metadata": {},
   "source": [
    "## 7.1 Introduction to RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2d61a-a373-4ab2-a02d-f79318c30cc1",
   "metadata": {},
   "source": [
    "- Retrieval-Augmented Generation (RAG) is a technique where an LLM retrieves relevant external knowledge (e.g., from documents or databases) and incorporates it into its prompt to generate more accurate and grounded responses.\n",
    "- A typical RAG application has two main components:\n",
    "\n",
    "    - **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happens offline_.\n",
    "\n",
    "    - **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    " \n",
    "### 7.1.1 Indexing\n",
    "\n",
    "We have looked at Indexing in an earlier tutorial (Semantic Search). The steps are summarised below:\n",
    "\n",
    "- **Load**: First we need to load our data. This is done with Document Loaders.\n",
    "- **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "- **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "### 7.1.2 Retrieval\n",
    "\n",
    "- **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "- **Generate**: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aeb11c-1fe3-4136-9683-dd347c7acd3b",
   "metadata": {},
   "source": [
    "## 7.2 RAG Over Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17888f4-b43a-4dd0-8f89-b317bd67f14f",
   "metadata": {},
   "source": [
    "### 7.2.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf7ff99-cd49-4a32-a38a-6f82f7c270bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "assert os.environ[\"LANGSMITH_TRACING\"] is not None\n",
    "assert os.environ[\"LANGSMITH_API_KEY\"] is not None\n",
    "assert os.environ[\"LANGSMITH_PROJECT\"] is not None\n",
    "assert os.environ[\"OPENAI_API_KEY\"] is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed397fb8-06e7-43c1-8ad8-d15cd038e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321db6b-791d-43db-a92b-7fa58d639643",
   "metadata": {},
   "source": [
    "## 7.2.2 Load Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ca431-a5e0-4250-96e3-e3b6ad4f2c6d",
   "metadata": {},
   "source": [
    "For this example, we will perform RAG over an HTML document.  We will use\n",
    "- `WebBaseLoader` to load HTML from web URLs\n",
    "- `BeautifulSoup` to parse it to text. In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2ed0cc-587c-4e10-b205-6ee7473481a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f799f1-1129-4807-9f05-1410d2f5216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc27a3-1044-4a3f-a3ea-8ca4c9ded1ca",
   "metadata": {},
   "source": [
    "### 7.2.3 Split Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434826f1-3b60-4781-8045-9edcd4e7cee7",
   "metadata": {},
   "source": [
    "Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89349552-e565-4865-8384-ce2eb9b270a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12ce21-3fed-4fd5-90ec-6acc5ff725c8",
   "metadata": {},
   "source": [
    "### 7.2.4 Storing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a634078-451e-40e3-9f61-fc206fe8114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3c1990b1-deff-4b09-a006-99b8c6d1b204', '293954f3-8f49-4a19-b270-fb2119e5add5', 'dbda4d37-fbd3-4dfc-8d5f-ccd4484d97ee']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb7489-093c-4000-8cdf-14ef360a1846",
   "metadata": {},
   "source": [
    "### 4.2.2 Streaming Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93356cb-7a06-42e7-b5b3-5057370c51a5",
   "metadata": {},
   "source": [
    "In addition to streaming back messages, it is also useful to stream back tokens. We can do this by specifying stream_mode=\"messages\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76881760-3311-4021-81be-49964d7a041f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, metadata \u001b[38;5;129;01min\u001b[39;00m agent_executor.stream(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [input_message]}, \u001b[43mconfig\u001b[49m, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m ):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata[\u001b[33m\"\u001b[39m\u001b[33mlanggraph_node\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33magent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (text := step.text()):\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mprint\u001b[39m(text, end=\u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "for step, metadata in agent_executor.stream(\n",
    "    {\"messages\": [input_message]}, config, stream_mode=\"messages\"\n",
    "):\n",
    "    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):\n",
    "        print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ee1f2-899c-4e7e-9785-f7df13d2382a",
   "metadata": {},
   "source": [
    "### 4.2.3 Adding Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8785f616-e349-4101-9308-6bb647e83a2e",
   "metadata": {},
   "source": [
    "In order for us to be able to chat with the agent, we need to add memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c49378c-cd51-4bf3-8cfa-ce1c12588579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Bob!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [(\"user\", \"Hi, I'm Bob!\")]}, config, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3013b0d0-efd1-49c7-abb9-4361874b1e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [(\"user\", \"What is my name?\")]}, config, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
