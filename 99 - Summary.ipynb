{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd88394-3b53-4ae8-9c5c-00573b01bfdd",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0f761-8f80-4ebf-b9a3-70431f3ac856",
   "metadata": {},
   "source": [
    "## 1. Documents\n",
    "\n",
    "- A Document is the unix of text that the LLM processes in order to answer queries. \n",
    "- A Document is not necessarly the whole file: files are typically chunked. Documents usually contain metadata including the name of the original document, the page from where the part of the document was extracted, author e.t.c.\n",
    "- Full texts are usually split into document chunks to optimise LLm output. Larger chunks = more stuff in context = more risk of hallunication. \n",
    "\n",
    "## 2. Embeddings\n",
    "\n",
    "- LLMs store text as vectors in a high-dimensional space\n",
    "- In this space, the position of each point (embedding) reflects the meaning of its corresponding text.\n",
    "Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.\n",
    "- In simpler words: LLMs store text in \"space\". The way that text is store relative to each other captures the relationship between the text. So text close to each other might have similar meaning. Text at a higher vertical axis might describe parent chld relationship. \n",
    " \n",
    "## 3. Vector Stores\n",
    "\n",
    "- A vector store is a specialised database for storing and querying embeddings. It supports standard operations like create, read, update, and delete (CRUD).\n",
    "- For retrieval, it allows searching for semantically similar text by comparing embedding vectors using similarity metrics such as cosine similarity.\n",
    "\n",
    "\n",
    "## 4. Tool Calling\n",
    "- Tool calling enables an LLM to interact with systems e.g. calling an API or querying a database.\n",
    "- When interacting with external tools, the request and response typically needs to confirm to a schema e.g. API Request Payload, SQL query structure.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import tool_example_to_messages\n",
    "\n",
    "class BudgetEntry(BaseModel):\n",
    "    amount: Optional[float] = Field(description = \"The income or expense amount\",default=0.0)\n",
    "    currency: Optional[str] = Field(description = \"The currency of the amount\",default='AED')\n",
    "    creditOrDebit: Optional[str] = Field(description = \"Credit or Debit. Debit if the amount was debited/spent. credit if the amount was received. Defaults to credit\", enum=[\"C\",\"D\"],default='D')\n",
    "    memo: Optional[str] = Field(description=\"Short description of the credit/debit event e.g. Shopping\")\n",
    "    category: str = Field(description=\"The category of the credit/debit event e.g. Bills\", enum=[\"Salary\",\"Bills\",\"Rent\",\"Shopping\",\"Car\",\"Home\"])\n",
    "\n",
    "class Extract(BaseModel):\n",
    "    entry:  Optional[BudgetEntry] = Field(description = \"The budget entry if all of required the details of the transaction were present in the text\"),\n",
    "    success: bool = Field(description=\"True/False value indicating if the text contained all required details for a transaction\")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        \"Fifty dollars for a t-shirt\",\n",
    "        Extract(success=True, entry=BudgetEntry(amount=50., currency=\"USD\",creditOrDebit=\"D\",memo=\"T-Shirt\",category=\"Shopping\")),\n",
    "    ),\n",
    "    (\n",
    "        \"And having the same one as six other people in this club is a hella don't\",\n",
    "        Extract(success=False, entry=None),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "messages = []\n",
    "\n",
    "for txt, tool_call in examples:\n",
    "    if tool_call.success:\n",
    "        # This final message is optional for some providers\n",
    "        ai_response = \"Detected entry.\"\n",
    "    else:\n",
    "        ai_response = \"Detected no entry.\"\n",
    "    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n",
    "\n",
    "message_with_extraction = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Apple Vision Pro thingy for $3999\",\n",
    "}\n",
    "\n",
    "# Add your few-shot examples + a new query\n",
    "response = model.invoke(messages + [message_with_extraction])\n",
    "\n",
    "structured_llm = model.with_structured_output(schema=Extract)\n",
    "structured_llm.invoke(messages + [message_with_extraction])\n",
    "```\n",
    "\n",
    "## 5. Structured Output\n",
    "\n",
    "- \n",
    "\n",
    "## 6. Few-shot Prompting\n",
    "- Few-shot prompting is a technique used with large language models (LLMs) where you provide a small number of examples (typically 1â€“5) within the prompt to show the model how to perform a task\n",
    "- Importantly, the examples should include \"negatives\" so that the LLM understands how to handle such cases.\n",
    "- **Different chat model providers impose different requirements for valid message sequences**\n",
    "\n",
    "## 7. Chatbots\n",
    "- By default, an LLM does not retain the context from previous invocations. For example, if you tell an LLM your name in one invocation, it will not \"remember\" your name in the subsequent invocations\n",
    "- For the LLM to remember the name, the chat history has to be sent with each invocation\n",
    "- Tools like LangGraph enable this.\n",
    "\n",
    "## 8. LangGraph\n",
    "- **LangGraph** is an open-source library from the creators of LangChain that is used to build A **stateful**, **multi-step** agent/workflow applications with reliable execution and **persistence**.\n",
    "- Crucially:\n",
    "    - LangChain updates the state after each human and AI interation and persists it in a vector store. This state acts as memory used by LLMs to give context-aware answers.\n",
    "    - Importantly, Human/Agent interactions are modeled as nodes in a graph which makes the orchestration between humans and agents visible and easy to debug.\n",
    "\n",
    "#### 8.1 Nodes\n",
    "- A node is a single action: e.g., call an LLM, run a tool, query a database, or invoke custom code.\n",
    "- Nodes can represent different actors (LLMs, tools, humans).\n",
    "- In case of a human, the graph encodes when to pause for human input, how to resume, and how actors exchange state.\n",
    "\n",
    "#### 8.2 Edges\n",
    "- Edges connect nodes and decide what runs next.\n",
    "- They can be unconditional, conditional (branching), looping, or fan-out/fan-in (parallel paths and joins).\n",
    "- Graph branches can run in parallel\n",
    "\n",
    "#### 8.3 Graph\n",
    "- A workflow is the directed graph of nodes and edges.\n",
    "- This makes the system explicit and debuggable: you can see the exact path the execution took.\n",
    "\n",
    "#### 8.4 State\n",
    "- The workflow carries a state object (typically a structured dict/schema) that persists across nodes.\n",
    "- Nodes read from state and propose updates to state rather than mutating it in place\n",
    "- After each node runs, LangGraph materialises a new state version.\n",
    "- This **immutability per step** yields reproducibility, diff-ability, and clear audit trails.\n",
    "\n",
    "#### 8.5 Checkpoints\n",
    "- Each step can be checkpointed (state snapshot + metadata).\n",
    "- Checkpoints and state can be stored in memory or external stores (DBs, object storage).\n",
    "\n",
    "- With checkpoints, you can retry or reroute without losing prior work.\n",
    "- This enables resume after failure/timeouts, deterministic replay, and precise debugging from any point.\n",
    "\n",
    "#### 8.6 Thread\n",
    "- A thread is one concrete run of the graph (e.g., a single user session).\n",
    "- Each thread has its own state history and checkpoints, isolating concurrent users/sessions cleanly.\n",
    "\n",
    "![](./docs/langgraph-checkpoints.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25317641-4fc9-4af1-9ee9-5ed5ba9c5a28",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # state['messages'] will be inserted in this placeholder.\n",
    "    ]\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages] # state stores messages\n",
    "    language: str # messages contain a variable 'language' whose value also needs to be stored with the mesages\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e7e8c-3025-4958-b812-a03e3bc1e631",
   "metadata": {},
   "source": [
    "## 9. Agents\n",
    "\n",
    "- An LLM agent is a system that uses a large language model to **autonomously decide which actions or tools to invoke in order to achieve a defined goal**.\n",
    "\n",
    "## 10. Tools\n",
    "\n",
    "- LLM tools are external functions or services that a large language model can callâ€”such as search, databases, or APIsâ€”to extend its capabilities beyond text generation.\n",
    "\n",
    "```python\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "search = TavilySearch(max_results=2)\n",
    "search_results = search.invoke(\"What is the weather in SF\")\n",
    "# Once we have all the tools we want, we can put them in a list that we will reference later.\n",
    "tools = [search]\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "agent_executor = create_react_agent(model, tools)\n",
    "input_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}\n",
    "response = agent_executor.invoke({\"messages\": [input_message]})\n",
    "\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd252c-d41c-40c8-9b1d-8e06cb8f8457",
   "metadata": {},
   "source": [
    "## 11. Retrieval Augmented Generation \n",
    "\n",
    "- Retrieval-Augmented Generation (RAG) is a technique where an LLM retrieves relevant external knowledge (e.g., from documents or databases) and incorporates it into its prompt to generate more accurate and grounded responses.\n",
    "- A typical RAG application has two main components:\n",
    "\n",
    "- **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happens offline_. The steps are:\n",
    "\n",
    "    - **Load**: First we need to load our data. This is done with Document Loaders.\n",
    "    - **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "    - **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "- **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "    - **Retrieve**:  A retriever is the component in a retrieval-augmented generation (RAG) system that fetches the most relevant information from an external knowledge source, given a user query. Instead of generating text, it returns raw documents or recordsâ€”often using vector similarity search, keyword search, or a hybrid approach. The retriever grounds the LLM by supplying fresh, domain-specific context, which reduces hallucination and ensures that generated answers are accurate and aligned with business data.\n",
    "    - **Generate**: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n",
    "\n",
    "```python\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7268e-dcaa-429e-9762-39446c8c444a",
   "metadata": {},
   "source": [
    "### 11.1 Query Analysis\n",
    "\n",
    "- Query Analysis is the process of employing models to transform or construct optimized search queries from raw user input.\n",
    "- For example, this could be transforming user input into an SQL query or a REST API request.\n",
    "\n",
    "\n",
    "```python\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "```\n",
    "\n",
    "The `analyze_query` node analyzes the user's prompt to map a search model that can be used to generate an SQL Query or an API request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728fe4-7d7a-4f11-9211-f7710949426a",
   "metadata": {},
   "source": [
    "### 11.2 RAG Chain\n",
    "- A RAG Chain is deterministic. It follows these 3 steps:\n",
    "\n",
    "    **Step 1** â†’ Call retriever with query.\n",
    "\n",
    "    **Step 2** â†’ Take retrieved docs + user query, feed them into LLM.\n",
    "\n",
    "    **Step 3** â†’ Return answer.\n",
    "\n",
    "In a RAG Chain the `retriever` is called a maximum of one time. This does not utilise the reasoning capabilities of the LLM. \n",
    "\n",
    "```python\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d61f49-c485-4642-bad5-39b21b943420",
   "metadata": {},
   "source": [
    "### 11.3 RAG Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38818e29-b46c-4fdb-8969-fbaff965a91d",
   "metadata": {},
   "source": [
    "- A Rag Agent is probabilistic.\n",
    "- The retriever is exposed as a tool.\n",
    "- The LLM decides how many times, and with what query.\n",
    "\n",
    "```python\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957bb54-42e8-4ab2-aa76-fc465e94bf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
