{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd88394-3b53-4ae8-9c5c-00573b01bfdd",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0f761-8f80-4ebf-b9a3-70431f3ac856",
   "metadata": {},
   "source": [
    "## 1. Documents\n",
    "\n",
    "- A Document is the unix of text that the LLM processes in order to answer queries. \n",
    "- A Document is not necessarly the whole file: files are typically chunked. Documents usually contain metadata including the name of the original document, the page from where the part of the document was extracted, author e.t.c.\n",
    "- Full texts are usually split into document chunks to optimise LLm output. Larger chunks = more stuff in context = more risk of hallunication. \n",
    "\n",
    "## 2. Embeddings\n",
    "\n",
    "- LLMs store text as vectors in a high-dimensional space\n",
    "- In this space, the position of each point (embedding) reflects the meaning of its corresponding text.\n",
    "Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.\n",
    "- In simpler words: LLMs store text in \"space\". The way that text is store relative to each other captures the relationship between the text. So text close to each other might have similar meaning. Text at a higher vertical axis might describe parent chld relationship. \n",
    " \n",
    "## 3. Vector Stores\n",
    "\n",
    "- A vector store is a specialised database for storing and querying embeddings. It supports standard operations like create, read, update, and delete (CRUD).\n",
    "- For retrieval, it allows searching for semantically similar text by comparing embedding vectors using similarity metrics such as cosine similarity.\n",
    "\n",
    "\n",
    "## 4. Tool Calling\n",
    "- Tool calling enables an LLM to interact with systems e.g. calling an API or querying a database.\n",
    "- When interacting with external tools, the request and response typically needs to confirm to a schema e.g. API Request Payload, SQL query structure.\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import tool_example_to_messages\n",
    "\n",
    "class BudgetEntry(BaseModel):\n",
    "    amount: Optional[float] = Field(description = \"The income or expense amount\",default=0.0)\n",
    "    currency: Optional[str] = Field(description = \"The currency of the amount\",default='AED')\n",
    "    creditOrDebit: Optional[str] = Field(description = \"Credit or Debit. Debit if the amount was debited/spent. credit if the amount was received. Defaults to credit\", enum=[\"C\",\"D\"],default='D')\n",
    "    memo: Optional[str] = Field(description=\"Short description of the credit/debit event e.g. Shopping\")\n",
    "    category: str = Field(description=\"The category of the credit/debit event e.g. Bills\", enum=[\"Salary\",\"Bills\",\"Rent\",\"Shopping\",\"Car\",\"Home\"])\n",
    "\n",
    "class Extract(BaseModel):\n",
    "    entry:  Optional[BudgetEntry] = Field(description = \"The budget entry if all of required the details of the transaction were present in the text\"),\n",
    "    success: bool = Field(description=\"True/False value indicating if the text contained all required details for a transaction\")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    (\n",
    "        \"Fifty dollars for a t-shirt\",\n",
    "        Extract(success=True, entry=BudgetEntry(amount=50., currency=\"USD\",creditOrDebit=\"D\",memo=\"T-Shirt\",category=\"Shopping\")),\n",
    "    ),\n",
    "    (\n",
    "        \"And having the same one as six other people in this club is a hella don't\",\n",
    "        Extract(success=False, entry=None),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "messages = []\n",
    "\n",
    "for txt, tool_call in examples:\n",
    "    if tool_call.success:\n",
    "        # This final message is optional for some providers\n",
    "        ai_response = \"Detected entry.\"\n",
    "    else:\n",
    "        ai_response = \"Detected no entry.\"\n",
    "    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n",
    "\n",
    "message_with_extraction = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Apple Vision Pro thingy for $3999\",\n",
    "}\n",
    "\n",
    "# Add your few-shot examples + a new query\n",
    "response = model.invoke(messages + [message_with_extraction])\n",
    "\n",
    "structured_llm = model.with_structured_output(schema=Extract)\n",
    "structured_llm.invoke(messages + [message_with_extraction])\n",
    "```\n",
    "\n",
    "## 5. Structured Output\n",
    "\n",
    "- \n",
    "\n",
    "## 6. Few-shot Prompting\n",
    "- Few-shot prompting is a technique used with large language models (LLMs) where you provide a small number of examples (typically 1â€“5) within the prompt to show the model how to perform a task\n",
    "- Importantly, the examples should include \"negatives\" so that the LLM understands how to handle such cases.\n",
    "- **Different chat model providers impose different requirements for valid message sequences**\n",
    "\n",
    "## 7. Chatbots\n",
    "- By default, an LLM does not retain the context from previous invocations. For example, if you tell an LLM your name in one invocation, it will not \"remember\" your name in the subsequent invocations\n",
    "- For the LLM to remember the name, the chat history has to be sent with each invocation\n",
    "- Tools like LangGraph enable this.\n",
    "\n",
    "## 8. LangGraph\n",
    "- **LangGraph** is an open-source library from the creators of LangChain that is used to build A **stateful**, **multi-step** agent/workflow applications with reliable execution and **persistence**.\n",
    "- Crucially:\n",
    "    - LangChain updates the state after each human and AI interation and persists it in a vector store. This state acts as memory used by LLMs to give context-aware answers.\n",
    "    - Importantly, Human/Agent interactions are modeled as nodes in a graph which makes the orchestration between humans and agents visible and easy to debug.\n",
    "\n",
    "#### 8.1 Nodes\n",
    "- A node is a single action: e.g., call an LLM, run a tool, query a database, or invoke custom code.\n",
    "- Nodes can represent different actors (LLMs, tools, humans).\n",
    "- In case of a human, the graph encodes when to pause for human input, how to resume, and how actors exchange state.\n",
    "\n",
    "#### 8.2 Edges\n",
    "- Edges connect nodes and decide what runs next.\n",
    "- They can be unconditional, conditional (branching), looping, or fan-out/fan-in (parallel paths and joins).\n",
    "- Graph branches can run in parallel\n",
    "\n",
    "#### 8.3 Graph\n",
    "- A workflow is the directed graph of nodes and edges.\n",
    "- This makes the system explicit and debuggable: you can see the exact path the execution took.\n",
    "\n",
    "#### 8.4 State\n",
    "- The workflow carries a state object (typically a structured dict/schema) that persists across nodes.\n",
    "- Nodes read from state and propose updates to state rather than mutating it in place\n",
    "- After each node runs, LangGraph materialises a new state version.\n",
    "- This **immutability per step** yields reproducibility, diff-ability, and clear audit trails.\n",
    "\n",
    "#### 8.5 Checkpoints\n",
    "- Each step can be checkpointed (state snapshot + metadata).\n",
    "- Checkpoints and state can be stored in memory or external stores (DBs, object storage).\n",
    "\n",
    "- With checkpoints, you can retry or reroute without losing prior work.\n",
    "- This enables resume after failure/timeouts, deterministic replay, and precise debugging from any point.\n",
    "\n",
    "#### 8.6 Thread\n",
    "- A thread is one concrete run of the graph (e.g., a single user session).\n",
    "- Each thread has its own state history and checkpoints, isolating concurrent users/sessions cleanly.\n",
    "\n",
    "![](./docs/langgraph-checkpoints.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25317641-4fc9-4af1-9ee9-5ed5ba9c5a28",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # state['messages'] will be inserted in this placeholder.\n",
    "    ]\n",
    ")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages] # state stores messages\n",
    "    language: str # messages contain a variable 'language' whose value also needs to be stored with the mesages\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e7e8c-3025-4958-b812-a03e3bc1e631",
   "metadata": {},
   "source": [
    "## 9. Agents\n",
    "\n",
    "- An LLM agent is a system that uses a large language model to **autonomously decide which actions or tools to invoke in order to achieve a defined goal**.\n",
    "\n",
    "## 10. Tools\n",
    "\n",
    "- LLM tools are external functions or services that a large language model can callâ€”such as search, databases, or APIsâ€”to extend its capabilities beyond text generation.\n",
    "\n",
    "```python\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "search = TavilySearch(max_results=2)\n",
    "search_results = search.invoke(\"What is the weather in SF\")\n",
    "# Once we have all the tools we want, we can put them in a list that we will reference later.\n",
    "tools = [search]\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\", model_provider=\"openai\")\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "agent_executor = create_react_agent(model, tools)\n",
    "input_message = {\"role\": \"user\", \"content\": \"Search for the weather in SF\"}\n",
    "response = agent_executor.invoke({\"messages\": [input_message]})\n",
    "\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd252c-d41c-40c8-9b1d-8e06cb8f8457",
   "metadata": {},
   "source": [
    "## 11. Retrieval Augmented Generation \n",
    "\n",
    "- Retrieval-Augmented Generation (RAG) is a technique where an LLM retrieves relevant external knowledge (e.g., from documents or databases) and incorporates it into its prompt to generate more accurate and grounded responses.\n",
    "- A typical RAG application has two main components:\n",
    "\n",
    "- **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happens offline_. The steps are:\n",
    "\n",
    "    - **Load**: First we need to load our data. This is done with Document Loaders.\n",
    "    - **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "    - **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "- **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "    - **Retrieve**:  A retriever is the component in a retrieval-augmented generation (RAG) system that fetches the most relevant information from an external knowledge source, given a user query. Instead of generating text, it returns raw documents or recordsâ€”often using vector similarity search, keyword search, or a hybrid approach. The retriever grounds the LLM by supplying fresh, domain-specific context, which reduces hallucination and ensures that generated answers are accurate and aligned with business data.\n",
    "    - **Generate**: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data\n",
    "\n",
    "```python\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7268e-dcaa-429e-9762-39446c8c444a",
   "metadata": {},
   "source": [
    "### 11.1 Query Analysis\n",
    "\n",
    "- Query Analysis is the process of employing models to transform or construct optimized search queries from raw user input.\n",
    "- For example, this could be transforming user input into an SQL query or a REST API request.\n",
    "\n",
    "\n",
    "```python\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "```\n",
    "\n",
    "The `analyze_query` node analyzes the user's prompt to map a search model that can be used to generate an SQL Query or an API request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728fe4-7d7a-4f11-9211-f7710949426a",
   "metadata": {},
   "source": [
    "### 11.2 RAG Chain\n",
    "- A RAG Chain is deterministic. It follows these 3 steps:\n",
    "\n",
    "    **Step 1** â†’ Call retriever with query.\n",
    "\n",
    "    **Step 2** â†’ Take retrieved docs + user query, feed them into LLM.\n",
    "\n",
    "    **Step 3** â†’ Return answer.\n",
    "\n",
    "In a RAG Chain the `retriever` is called a maximum of one time. This does not utilise the reasoning capabilities of the LLM. \n",
    "\n",
    "```python\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dbca8-3652-43d6-b299-c61b1caf094c",
   "metadata": {},
   "source": [
    "### 11.2.1 Human in the loop\n",
    "\n",
    "- We can add human intervention before sensitive steps (e.g. querying a database, creating or deleting files). \n",
    "- This gives a chance for a human to review the step and deciding whether to approve or reject.\n",
    "- This is enabled by LangGraph's persistence layer, which saves run progress to your storage of choice.\n",
    "```python\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])\n",
    "\n",
    "# Now that we're using persistence, we need to specify a thread ID\n",
    "# so that we can continue the run after review.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d61f49-c485-4642-bad5-39b21b943420",
   "metadata": {},
   "source": [
    "### 11.3 RAG Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38818e29-b46c-4fdb-8969-fbaff965a91d",
   "metadata": {},
   "source": [
    "- A Rag Agent is probabilistic.\n",
    "- The retriever is exposed as a tool.\n",
    "- The LLM decides how many times, and with what query.\n",
    "\n",
    "```python\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31cdaf6-01c0-4784-a024-b84dbe31cb6b",
   "metadata": {},
   "source": [
    "## 11.4 Working wih Databases\n",
    "\n",
    "- In order to perform RAG over Databases, we must turn the user's prompt into a query.\n",
    "\n",
    "### 11.4.1 Chains\n",
    "\n",
    "- With chains, our graph typically looks like `write_query` -> `execute_query` -> `generate_answer`.\n",
    "- To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain's structured output abstraction.\n",
    "    ```python\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "    system_message = \"\"\"\n",
    "    Given an input question, create a syntactically correct {dialect} query to\n",
    "    run to help find the answer. Unless the user specifies in his question a\n",
    "    specific number of examples they wish to obtain, always limit your query to\n",
    "    at most {top_k} results. You can order the results by a relevant column to\n",
    "    return the most interesting examples in the database.\n",
    "    \n",
    "    Never query for all the columns from a specific table, only ask for a the\n",
    "    few relevant columns given the question.\n",
    "    \n",
    "    Pay attention to use only the column names that you can see in the schema\n",
    "    description. Be careful to not query for columns that do not exist. Also,\n",
    "    pay attention to which column is in which table.\n",
    "    \n",
    "    Only use the following tables:\n",
    "    {table_info}\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = \"Question: {input}\"\n",
    "    \n",
    "    query_prompt_template = ChatPromptTemplate(\n",
    "        [(\"system\", system_message), (\"user\", user_prompt)]\n",
    "    )\n",
    "\n",
    "    class QueryOutput(TypedDict):\n",
    "        \"\"\"Generated SQL query.\"\"\"\n",
    "        query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "    \n",
    "    \n",
    "    def write_query(state: State):\n",
    "        \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "        prompt = query_prompt_template.invoke(\n",
    "            {\n",
    "                \"dialect\": db.dialect,\n",
    "                \"top_k\": 10,\n",
    "                \"table_info\": db.get_table_info(),\n",
    "                \"input\": state[\"question\"],\n",
    "            }\n",
    "        )\n",
    "        structured_llm = llm.with_structured_output(QueryOutput)\n",
    "        result = structured_llm.invoke(prompt)\n",
    "        return {\"query\": result[\"query\"]}\n",
    "    ```\n",
    "\n",
    "- To execute the query, we will use the [`QuerySQLDatabaseTool`](). It is recommended to add a human in the looop before executing a SQL query\n",
    "    ```python\n",
    "    from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
    "\n",
    "    def execute_query(state: State):\n",
    "        \"\"\"Execute SQL query.\"\"\"\n",
    "        execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
    "        return {\"result\": execute_query_tool.invoke(state[\"query\"])}\n",
    "    ```\n",
    "\n",
    "### 11.4.2 Agents\n",
    "\n",
    "The advantage of using Agents to perform RAG over Databases is that it leverage the reasoning capabilities of LLMs to make decisions during execution. This includes:\n",
    "\n",
    "- They can query the database as many times as needed to answer the user question\n",
    "- They can recover from errors by running a generated query, catching the traceback and regenerating it correctly\n",
    "- They can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table).\n",
    "\n",
    "To build an agent, we can leverage LangChain's [SQLDatabaseToolkit](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.sql.toolkit.SQLDatabaseToolkit.html).\n",
    "\n",
    "The SQLDatabaseToolkit includes tools that can:\n",
    "\n",
    "    - Create and execute queries\n",
    "    - Check query syntax\n",
    "    - Retrieve table descriptions\n",
    "    - ... and more\n",
    "    \n",
    "    ```python\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "    toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "    tools = toolkit.get_tools()\n",
    "\n",
    "    agent_executor = create_react_agent(llm, tools, prompt=system_message)\n",
    "    question = \"Which country's customers spent the most?\"\n",
    "\n",
    "    for step in agent_executor.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        step[\"messages\"][-1].pretty_print()\n",
    "    ```\n",
    "\n",
    "### 11.4.3 Dealing with high cardinality columns\n",
    "\n",
    "- High Cardinality columns are columns that contain many distinct values. \n",
    "    - A `full_name` column is an example of a high cardinality column\n",
    "    - `is_verified` is a low cardinality column.\n",
    "\n",
    "- In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.\n",
    "\n",
    "- We can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word\n",
    "\n",
    "```\n",
    "import ast\n",
    "import re\n",
    "\n",
    "\n",
    "def query_as_list(db, query):\n",
    "    res = db.run(query)\n",
    "    res = [el for sub in ast.literal_eval(res) for el in sub if el]\n",
    "    res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]\n",
    "    return list(set(res))\n",
    "\n",
    "\n",
    "artists = query_as_list(db, \"SELECT Name FROM Artist\")\n",
    "albums = query_as_list(db, \"SELECT Title FROM Album\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = vector_store.add_texts(artists + albums)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "description = (\n",
    "    \"Use to look up values to filter on. Input is an approximate spelling \"\n",
    "    \"of the proper noun, output is valid proper nouns. Use the noun most \"\n",
    "    \"similar to the search.\"\n",
    ")\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"search_proper_nouns\",\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "# Add to system message\n",
    "suffix = (\n",
    "    \"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \"\n",
    "    \"the filter value using the 'search_proper_nouns' tool! Do not try to \"\n",
    "    \"guess at the proper name - use this function to find similar ones.\"\n",
    ")\n",
    "\n",
    "system = f\"{system_message}\\n\\n{suffix}\"\n",
    "\n",
    "tools.append(retriever_tool)\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt=system)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6374aa2-989b-470f-8370-151fad77222e",
   "metadata": {},
   "source": [
    "## 12. Summarization\n",
    "\n",
    "There are three building a summarizer: \n",
    "\n",
    "1. **Stuffing the prompt** If the document fits in to the context window, we can just include the document in the prompt. This is the simplest approach. This is implemented using `create_stuff_documents_chain`\n",
    "\n",
    "2. **Map Reduce**: If the document(s) do not fit in to the context window, we **map each document into a summary** and then **reduce the summaries of each document into one final summary**. The mapping step is typically done in parallel. This is implemented usinf the [`MapReduceDocumentsChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html)\n",
    "\n",
    "3. **Iterative Refinement**: Iterative Refinement builds on Map-Reduce. Map-Reduce is effective when the documents do not have a sequential nature. In other cases, such as summarizing a novel or body of text with an inherent sequence, [iterative refinement](https://python.langchain.com/docs/how_to/summarize_refine/) may be more effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
